"""
Local Models Integration with Langchain

Integrates BGE-M3 embeddings and Ollama LLM for local processing without API keys.
"""

import os
os.environ["NO_PROXY"] = "localhost,127.0.0.1,::1"

import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger

try:
    from langchain_ollama import ChatOllama
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import JsonOutputParser
    from langchain_core.documents import Document
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.messages import HumanMessage
except ImportError:
    raise

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    logger.warning("sentence-transformers not installed. Please run: pip install sentence-transformers")
    SentenceTransformer = None


class LocalEmbeddingModel:
    """BGE-M3 local embedding model using sentence-transformers."""
    
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        """Initialize BGE-M3 embedding model."""
        self.model_name = model_name
        self.model = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the embedding model."""
        try:
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            logger.info("âœ“ BGE-M3 embedding model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            raise
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents using encode_document for IR tasks."""
        if not self.model:
            raise RuntimeError("Model not initialized")
            
        logger.debug(f"Embedding {len(texts)} documents...")
        # Use encode_document for information retrieval tasks as recommended in v5
        try:
            embeddings = self.model.encode_document(texts, normalize_embeddings=True)
        except AttributeError:
            # Fallback to encode if encode_document is not available
            logger.debug("encode_document not available, falling back to encode")
            embeddings = self.model.encode(texts, normalize_embeddings=True)
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query using encode_query for IR tasks."""
        if not self.model:
            raise RuntimeError("Model not initialized")
            
        # Use encode_query for information retrieval tasks as recommended in v5
        try:
            embedding = self.model.encode_query(text, normalize_embeddings=True)
            return embedding.tolist()
        except AttributeError:
            # Fallback to encode if encode_query is not available
            logger.debug("encode_query not available, falling back to encode")
            embedding = self.model.encode([text], normalize_embeddings=True)
            return embedding[0].tolist()


class LocalLLMModel:
    """Local LLM using Ollama."""
    
    def __init__(
        self, 
        model_name: str = "llama3.1:8b",  # æˆ–è€…ç”¨æˆ·å®é™…ä½¿ç”¨çš„æ¨¡å‹å
        ollama_base_url: str = "http://localhost:11434",
        temperature: float = 0.1,
        max_tokens: int = 4000
    ):
        """Initialize Ollama LLM."""
        self.model_name = model_name
        self.ollama_base_url = ollama_base_url
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.llm = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """Initialize the Ollama LLM."""
        try:
            logger.info(f"Connecting to Ollama at {self.ollama_base_url}")
            self.llm = ChatOllama(
                model=self.model_name,
                base_url=self.ollama_base_url,
                temperature=self.temperature,
                num_predict=self.max_tokens
            )
            
            # Test connection
            test_msg = self.llm.invoke("Hello")
            test_text = getattr(test_msg, "content", str(test_msg))
            logger.info(f"âœ“ Ollama chat model ({self.model_name}) connected successfully")
            logger.debug(f"Test response: {test_text[:100]}...")
            
        except Exception as e:
            logger.error(f"Failed to connect to Ollama: {e}")
            logger.error("Make sure Ollama is running: ollama serve")
            logger.error(f"And model is available: ollama pull {self.model_name}")
            raise
    
    def generate(self, prompt: str) -> str:
        """Generate text using the local Chat LLM."""
        if not self.llm:
            raise RuntimeError("LLM not initialized")    
        try:
            message = HumanMessage(content=prompt)
            response = self.llm.invoke([message])
            return response.content.strip()
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            raise
    
    def batch_generate(self, prompts: List[str]) -> List[str]:
        """Generate text for multiple prompts."""
        results = []
        for prompt in prompts:
            try:
                result = self.generate(prompt)
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to generate for prompt: {e}")
                results.append(f"Generation failed: {e}")
        
        return results


class LangchainSummaryChain:
    """Langchain-based summary generation chain."""
    
    def __init__(self, llm_model: LocalLLMModel):
        """Initialize summary chain."""
        self.llm = llm_model
        self._setup_chains()
    
    def _setup_chains(self):
        """Setup the Langchain summary chains using new LCEL syntax."""
        # Function summary prompt template
        self.summary_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå‡½æ•°ä»£ç å¹¶ç”Ÿæˆç»“æ„åŒ–çš„JSONæ‘˜è¦ã€‚

ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šã€æ ‡è®°æˆ–æ–‡æœ¬ï¼š
{{
  "summary": "1-2å¥è¯ç®€è¿°å‡½æ•°åŠŸèƒ½",
  "purpose": "è¯¦ç»†è¯´æ˜å‡½æ•°çš„ç›®çš„å’Œä½œç”¨", 
  "parameters": [
    {{"name": "å‚æ•°å", "type": "ç±»å‹", "description": "å‚æ•°ä½œç”¨"}}
  ],
  "returns": "è¿”å›å€¼æè¿°",
  "dependencies": ["ä¾èµ–çš„å†…éƒ¨å‡½æ•°åˆ—è¡¨"],
  "complexity": "LOW|MEDIUM|HIGH"
}}

åˆ†æè¦æ±‚ï¼š
1. ä½¿ç”¨ä¸­æ–‡æè¿°
2. é‡ç‚¹å…³æ³¨å‡½æ•°çš„ä¸šåŠ¡é€»è¾‘ï¼Œè€Œéå®ç°ç»†èŠ‚  
3. åªåˆ—å‡ºé¡¹ç›®å†…éƒ¨å‡½æ•°ä¾èµ–ï¼Œä¸åŒ…æ‹¬åº“å‡½æ•°
4. å¤æ‚åº¦åŸºäºé€»è¾‘å¤æ‚æ€§å’Œä»£ç è¡Œæ•°åˆ¤æ–­"""),
            ("human", """è¯·åˆ†æä»¥ä¸‹å‡½æ•°å¹¶æä¾›ç»“æ„åŒ–çš„JSONæ‘˜è¦ï¼š

å‡½æ•°ä¿¡æ¯ï¼š
- åç§°: {function_name}
- å®Œæ•´è·¯å¾„: {qualified_name}
- æ–‡æ¡£å­—ç¬¦ä¸²: {docstring}

æºä»£ç ï¼š
```python
{source_code}
```

ä¾èµ–ä¸Šä¸‹æ–‡ï¼š
{dependencies_context}

è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š""")
        ])
        
        # Batch summary prompt for multiple functions
        self.batch_summary_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æä¸“å®¶ã€‚ä½ éœ€è¦åˆ†æå¤šä¸ªå‡½æ•°å¹¶ä¸ºæ¯ä¸ªå‡½æ•°ç”Ÿæˆç»“æ„åŒ–çš„JSONæ‘˜è¦ã€‚

ä½ éœ€è¦ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šã€æ ‡è®°æˆ–æ–‡æœ¬ï¼š
[
  {{
    "qualified_name": "å‡½æ•°å®Œæ•´è·¯å¾„",
    "summary": "1-2å¥è¯ç®€è¿°åŠŸèƒ½",
    "purpose": "è¯¦ç»†ç›®çš„è¯´æ˜",
    "parameters": [{{"name": "å‚æ•°å", "type": "ç±»å‹", "description": "è¯´æ˜"}}],
    "returns": "è¿”å›å€¼æè¿°", 
    "dependencies": ["å†…éƒ¨å‡½æ•°ä¾èµ–"],
    "complexity": "LOW|MEDIUM|HIGH"
  }}
]

åˆ†æè¦æ±‚ï¼š
1. ä½¿ç”¨ä¸­æ–‡æè¿°
2. åŸºäºä¸Šä¸‹æ–‡åº“ç†è§£å‡½æ•°ä¾èµ–å…³ç³»
3. åªåˆ—å‡ºé¡¹ç›®å†…éƒ¨å‡½æ•°ä¾èµ–ï¼Œä¸åŒ…æ‹¬åº“å‡½æ•°
4. æŒ‰æä¾›çš„å‡½æ•°é¡ºåºè¿”å›ç»“æœ"""),
            ("human", """è¯·åˆ†æä»¥ä¸‹å¤šä¸ªå‡½æ•°å¹¶ä¸ºæ¯ä¸ªå‡½æ•°æä¾›ç»“æ„åŒ–çš„JSONæ‘˜è¦ï¼š

ä¸Šä¸‹æ–‡åº“ï¼ˆä¾èµ–å‡½æ•°çš„æ‘˜è¦ï¼‰ï¼š
{context_library}

å¾…åˆ†æçš„å‡½æ•°ï¼š
{functions_data}

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š""")
        ])
        
        # Create chains using LCEL syntax with JSON parsing
        self.json_parser = JsonOutputParser()
        self.summary_chain = self.summary_prompt | self.llm.llm | self.json_parser
        self.batch_summary_chain = self.batch_summary_prompt | self.llm.llm | self.json_parser
    
    def generate_single_summary(
        self, 
        function_name: str,
        qualified_name: str,
        source_code: str,
        docstring: str = "",
        dependencies_context: str = ""
    ) -> Dict[str, Any]:
        """Generate summary for a single function."""
        try:
            result = self.summary_chain.invoke({
                "function_name": function_name,
                "qualified_name": qualified_name,
                "source_code": source_code,
                "docstring": docstring or "æ— æ–‡æ¡£å­—ç¬¦ä¸²",
                "dependencies_context": dependencies_context or "æ— ä¾èµ–"
            })
            return result
            
        except Exception as e:
            logger.error(f"Single summary generation failed: {e}")
            raise
    
    def generate_batch_summaries(
        self,
        functions_data: str,
        context_library: str
    ) -> List[Dict[str, Any]]:
        """Generate summaries for multiple functions."""
        try:
            result = self.batch_summary_chain.invoke({
                "functions_data": functions_data,
                "context_library": context_library or "æš‚æ— ä¾èµ–ä¸Šä¸‹æ–‡"
            })
            return result
            
        except Exception as e:
            logger.error(f"Batch summary generation failed: {e}")
            raise


def test_local_models():
    """Test local models functionality."""
    print("ğŸ§ª Testing Local Models...")
    
    # Test embedding model
    try:
        print("Testing BGE-M3 embedding...")
        embedding_model = LocalEmbeddingModel()
        
        test_texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å‡½æ•°", "ç”¨äºç”¨æˆ·è®¤è¯çš„æ–¹æ³•"]
        embeddings = embedding_model.embed_documents(test_texts)
        print(f"âœ“ Generated {len(embeddings)} embeddings of dimension {len(embeddings[0])}")
        
    except Exception as e:
        print(f"âŒ Embedding test failed: {e}")
        return False
    
    # Test LLM model
    try:
        print("Testing Ollama LLM...")
        llm_model = LocalLLMModel()
        
        test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€ã€‚"
        response = llm_model.generate(test_prompt)
        print(f"âœ“ LLM response: {response[:100]}...")
        
    except Exception as e:
        print(f"âŒ LLM test failed: {e}")
        return False
    
    # Test summary chain
    try:
        print("Testing summary chain...")
        summary_chain = LangchainSummaryChain(llm_model)
        
        test_summary = summary_chain.generate_single_summary(
            function_name="test_function",
            qualified_name="module.test_function",
            source_code="def test_function():\n    return 'hello'",
            docstring="æµ‹è¯•å‡½æ•°"
        )
        print(f"âœ“ Summary generated: {test_summary[:100]}...")
        
    except Exception as e:
        print(f"âŒ Summary chain test failed: {e}")
        return False
    
    print("ğŸ‰ All local model tests passed!")
    return True


if __name__ == "__main__":
    test_local_models()