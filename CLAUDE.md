# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is an intelligent Code RAG (Retrieval-Augmented Generation) system that combines AST parsing with AI-powered knowledge extraction and hybrid search capabilities. The system:

1. **Parses codebases** using Tree-sitter to build knowledge graphs in Memgraph
2. **Generates intelligent summaries** of functions using dependency-aware topological processing  
3. **Builds hybrid search indices** combining vector embeddings (Weaviate) and keyword search (BM25)
4. **Provides intelligent code search** with semantic understanding and contextual retrieval

The system transforms raw code into searchable, semantically-rich knowledge that can answer complex queries about code functionality, relationships, and purpose.

## Architecture

The project follows a modular architecture with distinct processing phases:

### Core Components

- **GraphUpdater** (`code_parser/graph_updater.py`): Main orchestrator that coordinates the parsing process through three phases:
  1. Structure identification (packages, folders)
  2. File processing and AST caching with definition collection
  3. Function call processing using cached ASTs

- **ProcessorFactory** (`code_parser/parsers/factory.py`): Dependency injection container that creates processor instances with proper dependencies

- **MemgraphIngestor** (`code_parser/services/graph_service.py`): Handles all database communication with batching and buffering for performance

### Processor Pipeline

1. **StructureProcessor**: Identifies packages, folders, and project structure
2. **DefinitionProcessor**: Extracts functions, classes, methods from source files
3. **CallProcessor**: Analyzes function calls and method invocations
4. **ImportProcessor**: Tracks import relationships between modules
5. **TypeInferenceEngine**: Infers types for better call resolution

### Key Data Structures

- **FunctionRegistryTrie**: Optimized trie for function qualified name lookups with prefix/suffix search capabilities
- **AST Cache**: Stores parsed Tree-sitter ASTs for reuse during call processing phase
- **Simple Name Lookup**: Maps simple function names to qualified names for resolution

## Enhanced Knowledge Engine

The system includes an intelligent knowledge extraction and retrieval layer built on top of the AST parsing infrastructure:

### Knowledge Engine Components

- **DependencyGraphBuilder** (`knowledge_engine/dependency_graph.py`): Constructs function call dependency graphs and performs topological sorting for dependency-aware processing

- **TopologicalSummaryGenerator** (`knowledge_engine/topological_summary.py`): Generates intelligent function summaries using LLM in dependency order, ensuring all called functions are summarized before their callers

- **DualEngineIndexer** (`knowledge_engine/dual_indexer.py`): Builds hybrid search indices combining vector embeddings (Weaviate) and keyword search (BM25) for comprehensive retrieval

- **RRFRetriever** (`knowledge_engine/rrf_retriever.py`): Implements Reciprocal Rank Fusion for combining semantic and keyword search results with intelligent query analysis

- **EnhancedGraphUpdater** (`enhanced_graph_updater.py`): Orchestrates the complete pipeline from AST parsing through knowledge extraction to search index construction

### Processing Pipeline

The enhanced system follows a 7-phase pipeline:

1. **Structure Identification**: Original AST parsing (packages, folders)
2. **File Processing**: Original AST parsing (functions, classes, caching)  
3. **Call Analysis**: Original AST parsing (function calls, relationships)
4. **Dependency Graph**: Build function call dependency graph with cycle detection
5. **Summary Generation**: LLM-powered function summarization in topological order
6. **Index Construction**: Build vector and BM25 search indices
7. **Retriever Initialization**: Setup hybrid search with RRF fusion

### Search Capabilities

- **Semantic Search**: Vector similarity using OpenAI embeddings
- **Keyword Search**: BM25-based exact term matching
- **Hybrid Fusion**: RRF combination with query-adaptive weighting
- **Similarity Search**: Find functions similar to a reference function
- **Complexity Filtering**: Search by function complexity levels
- **Context-Aware Results**: Results include summaries, purposes, and dependencies

## Common Development Tasks

### Enhanced System Usage

#### ğŸ”¥ æœ¬åœ°ç‰ˆæœ¬ (æ¨è - æ— éœ€ API key)

```bash
# å®‰è£…æœ¬åœ°ç‰ˆæœ¬ä¾èµ–
pip install -r requirements-local.txt

# å¯åŠ¨æœ¬åœ°æœåŠ¡
docker-compose -f docker-compose-local.yml up -d

# å®‰è£…å’Œé…ç½® Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull gpt-oss-20b  # æˆ–å…¶ä»–æœ¬åœ°æ¨¡å‹

# è¿è¡Œå®Œæ•´æœ¬åœ°æµæ°´çº¿
python local_enhanced_main.py --repo-path /path/to/your/codebase

# äº¤äº’å¼æœ¬åœ°æœç´¢
python local_enhanced_main.py --interactive

# æœ¬åœ°æœç´¢å‘½ä»¤
python query_interface.py --search "ç”¨æˆ·è®¤è¯ç›¸å…³å‡½æ•°"
python query_interface.py --similar "myproject.auth.login"
```

#### OpenAI ç‰ˆæœ¬

```bash
# Install dependencies
pip install -r requirements.txt

# Set up environment variables
export OPENAI_API_KEY="your-openai-api-key"

# Start required services (Memgraph + Weaviate)
docker run -p 7687:7687 -p 7444:7444 -p 3000:3000 memgraph/memgraph-platform
docker run -p 8080:8080 semitechnologies/weaviate:latest

# Run complete enhanced pipeline (AST + Knowledge Engine)
python enhanced_main.py --repo-path /path/to/your/codebase

# Interactive search interface
python query_interface.py --interactive
```

#### åŸå§‹ç‰ˆæœ¬ (ä»… AST è§£æ)

```bash
# Run AST parsing only (original functionality)
python main.py
```

### System Requirements

#### æœ¬åœ°ç‰ˆæœ¬ (æ¨è)
- **Python 3.12+** with packages (see `requirements-local.txt`)
- **Memgraph Database**: Graph database for AST relationships (port 7687)
- **Weaviate Database**: Vector database for semantic search (port 8080)
- **Ollama**: æœ¬åœ° LLM æœåŠ¡ (port 11434)
  - å®‰è£…: https://ollama.ai/
  - æ¨¡å‹: `ollama pull llama3.1:8b` (æˆ–gemma2:9b)
- **BGE-M3**: è‡ªåŠ¨ä¸‹è½½çš„ä¸­æ–‡ä¼˜åŒ–åµŒå…¥æ¨¡å‹

#### OpenAI ç‰ˆæœ¬
- **Python 3.12+** with packages (see `requirements.txt`)
- **Memgraph Database**: Graph database for AST relationships (port 7687)
- **Weaviate Database**: Vector database for semantic search (port 8080)
- **OpenAI API Key**: For LLM summaries and embeddings (set `OPENAI_API_KEY`)

### Entry Points

**Enhanced Versions:**
- `local_enhanced_main.py`: ğŸ”¥ **æœ¬åœ°ç‰ˆæœ¬** - ä½¿ç”¨ Langchain + BGE-M3 + Ollamaï¼Œæ— éœ€ API key
- `enhanced_main.py`: OpenAI ç‰ˆæœ¬ - ä½¿ç”¨ OpenAI API

**Original Version:**
- `main.py`: åŸå§‹ AST è§£æåŠŸèƒ½

**Interactive Interface:**
- `query_interface.py`: äº¤äº’å¼æœç´¢ç•Œé¢ï¼ˆæ”¯æŒä¸¤ä¸ªç‰ˆæœ¬ï¼‰

### Configuration

Configuration is handled through `code_parser/config.py` using Pydantic settings and environment variables:

- **AST Parsing**: Memgraph connection settings (localhost:7687), ignore patterns
- **Knowledge Engine**: OpenAI API settings, Weaviate connection (localhost:8080)
- **Models**: OpenAI GPT-3.5/4 for summaries, text-embedding-3-small for vectors
- **Search**: RRF fusion parameters, batch processing limits

Environment variables:
```bash
OPENAI_API_KEY=your-key-here        # Required for knowledge engine features
MEMGRAPH_HOST=localhost             # Memgraph host (default: localhost) 
WEAVIATE_URL=http://localhost:8080  # Weaviate URL (default: localhost:8080)
```

### Data Storage and Outputs

The system generates several artifacts:

- **Memgraph Database**: Function definitions, call relationships, project structure
- **Weaviate Database**: Vector embeddings for semantic search
- **JSON Exports**: Function summaries, dependency graphs, search indices
- **Knowledge Base**: Exported to `knowledge_base_export/` directory

Access Memgraph Lab at http://localhost:3000 to visualize the generated knowledge graph.

### Language Support

The system uses Tree-sitter parsers located in `code_parser/grammars/` and supports multiple programming languages through `language_config.py`. Parser loading is handled by `parser_loader.py`.

## File Structure Patterns

```
â”œâ”€â”€ code_parser/              # Original AST parsing infrastructure
â”‚   â”œâ”€â”€ parsers/             # Core parsing logic with modular processors  
â”‚   â”œâ”€â”€ services/            # Database and external service integrations
â”‚   â””â”€â”€ grammars/            # Tree-sitter grammar repositories
â”œâ”€â”€ knowledge_engine/        # Enhanced knowledge extraction and search
â”‚   â”œâ”€â”€ dependency_graph.py  # Function dependency analysis
â”‚   â”œâ”€â”€ topological_summary.py # OpenAI LLM summary generation
â”‚   â”œâ”€â”€ dual_indexer.py      # OpenAI embedding + hybrid search
â”‚   â”œâ”€â”€ local_models.py      # ğŸ”¥ Langchain + local model integration
â”‚   â”œâ”€â”€ local_topological_summary.py  # ğŸ”¥ Ollama LLM summary generation  
â”‚   â”œâ”€â”€ local_dual_indexer.py # ğŸ”¥ BGE-M3 embedding + hybrid search
â”‚   â””â”€â”€ rrf_retriever.py     # Search and result fusion
â”œâ”€â”€ enhanced_graph_updater.py # OpenAI version pipeline orchestrator
â”œâ”€â”€ local_enhanced_graph_updater.py # ğŸ”¥ Local version pipeline orchestrator
â”œâ”€â”€ query_interface.py       # Interactive search interface (supports both)
â”œâ”€â”€ enhanced_main.py         # OpenAI version entry point
â”œâ”€â”€ local_enhanced_main.py   # ğŸ”¥ Local version entry point (no API key)
â”œâ”€â”€ main.py                  # Original AST parsing entry point
â”œâ”€â”€ requirements.txt         # OpenAI version dependencies
â”œâ”€â”€ requirements-local.txt   # ğŸ”¥ Local version dependencies
â”œâ”€â”€ docker-compose.yml       # OpenAI version services
â”œâ”€â”€ docker-compose-local.yml # ğŸ”¥ Local version services
â””â”€â”€ README-LOCAL.md          # ğŸ”¥ Local version detailed guide
```

## Development Notes

### AST Parsing Layer (Original)
- Three-pass processing: structure â†’ definitions â†’ calls 
- AST caching prevents redundant parsing during call analysis
- Batched database writes improve ingestion performance
- Function registry uses trie data structure for efficient qualified name lookups
- Ignore patterns exclude common directories (node_modules, .git, __pycache__, etc.)

### Knowledge Engine Layer (Enhanced)  
- **Dependency-Aware Processing**: Functions summarized in topological order ensuring dependencies are processed first
- **Circular Dependency Handling**: Strong Connected Components (SCCs) detected and processed as units
- **Batch Processing**: LLM calls batched with token estimation to optimize API usage and costs
- **Hybrid Search**: Combines vector similarity (semantic) with BM25 (keyword) using RRF fusion
- **Query Analysis**: Automatic query type detection adjusts search strategy weights
- **Error Recovery**: Fallback mechanisms handle LLM failures without stopping pipeline
- **Version Compatibility**: Supports both new LangChain 0.3+ (LCEL) and legacy versions automatically

### Performance Considerations
- **Token Management**: Batch size dynamically adjusted based on token counts
- **Rate Limiting**: Built-in delays between LLM API calls
- **Caching Strategy**: AST cache reused across processing phases
- **Memory Management**: Large codebases processed in dependency-ordered batches
- **Index Optimization**: Both vector and keyword indices optimized for retrieval speed

### Architecture Principles
- **Separation of Concerns**: AST parsing and knowledge extraction as separate layers
- **Dependency Injection**: ProcessorFactory provides clean component dependencies  
- **Error Boundaries**: Each processing phase isolated with fallback strategies
- **Extensibility**: Modular design allows adding new search engines or LLM providers

## Testing and Quality

MVP implementation focuses on core functionality. Recommended additions:
- Unit tests for each knowledge engine component
- Integration tests for end-to-end pipeline
- Search relevance evaluation metrics
- Performance benchmarking for large codebases

## åº“ç”¨æ³•æ ¡éªŒè§„åˆ™ï¼ˆAPI ä½¿ç”¨å‰å¿…é¡»æ‰§è¡Œï¼‰
- åœ¨ä½¿ç”¨ä»»ä½•ç¬¬ä¸‰æ–¹åº“/æ¡†æ¶å‰ï¼Œå…ˆæ‰§è¡Œâ€œç‰ˆæœ¬ä¸æ–‡æ¡£ç¡®è®¤â€ï¼š
  1) ç¡®è®¤ç›®æ ‡åº“çš„**ä¸»ç‰ˆæœ¬å·**ï¼›è‹¥ç”¨æˆ·æœªæŒ‡å®šï¼Œåˆ™é€‰ç”¨**æœ€æ–°ç¨³å®šç‰ˆæœ¬**å¹¶åœ¨è¾“å‡ºä¸­æ ‡è®°ç‰ˆæœ¬å·ã€‚
  2) é€šè¿‡ WebSearch/WebFetch æŸ¥æ‰¾è¯¥ç‰ˆæœ¬çš„**å®˜æ–¹æ–‡æ¡£/è¿ç§»æŒ‡å—/å‘å¸ƒæ—¥å¿—**ï¼Œè¯†åˆ«æ˜¯å¦æœ‰å¼ƒç”¨/ç ´åå¼å˜æ›´ã€‚
  3) ä»…é‡‡ç”¨**å®˜æ–¹æ¨è**çš„å†™æ³•ï¼›è‹¥æˆ‘æœ€åˆçš„æ–¹æ¡ˆä¸æ¨èä¸ä¸€è‡´ï¼Œå¿…é¡»æ”¹ä¸ºæ¨èå†™æ³•å¹¶è§£é‡ŠåŸå› ã€‚