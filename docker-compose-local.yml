version: '3.8'

services:
  memgraph:
    image: memgraph/memgraph-platform:latest
    container_name: code-rag-local-memgraph
    ports:
      - "7687:7687"  # Bolt protocol
      - "7444:7444"  # HTTP
      - "3000:3000"  # Memgraph Lab
    environment:
      - MEMGRAPH="--log-level=INFO"
    volumes:
      - memgraph-local-data:/var/lib/memgraph
      - memgraph-local-log:/var/log/memgraph
      - memgraph-local-etc:/etc/memgraph
    restart: unless-stopped

  weaviate:
    image: semitechnologies/weaviate:1.22.4
    container_name: code-rag-local-weaviate
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'  # We provide our own vectors (BGE-M3)
      ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid'
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate-local-data:/var/lib/weaviate
    restart: unless-stopped

volumes:
  memgraph-local-data:
  memgraph-local-log:
  memgraph-local-etc:
  weaviate-local-data:

# Note: Ollama should be installed and run separately on the host system
# 1. Install Ollama: https://ollama.ai/
# 2. Start Ollama: ollama serve
# 3. Pull your model: ollama pull gpt-oss-20b
# 4. Ollama will be accessible at http://localhost:11434